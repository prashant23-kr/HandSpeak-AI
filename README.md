# ğŸ¤Ÿ HandSpeak-AI

HandSpeak-AI is an **AI-powered hand gesture and sign language recognition system** that uses **computer vision and deep learning** to interpret hand gestures and convert them into meaningful outputs.  
The goal of this project is to **bridge communication gaps** between hearing-impaired individuals and others.

---

## ğŸ“Œ Features
- Real-time hand gesture recognition  
- AI-based gesture data collection  
- Supports sign language interpretation  
- Scalable for future speech/text conversion  
- Simple and modular Python codebase  

---

## ğŸ§  Technologies Used
- **Python**
- **OpenCV**
- **NumPy**
- **Machine Learning / Deep Learning**
- **Computer Vision**

---

## ğŸ“‚ Project Structure
HandSpeak-AI/
â”‚â”€â”€ datacollection.py # Script to collect hand gesture data
â”‚â”€â”€ test.py # Testing and inference script
â”‚â”€â”€ README.md # Project documentation
â”‚â”€â”€ LICENSE # MIT License
â”‚â”€â”€ .gitignore # Ignored files and folders
ğŸ“Š Dataset

The dataset used for gesture recognition is not included in this repository due to size limitations.
ğŸš€ Future Enhancements

1-Convert recognized gestures into text
2-Add speech output
3-Improve model accuracy
4-Deploy as a web or mobile application
5-Support multiple sign languages
6-Detect medicine 

ğŸ‘¨â€ğŸ’» Author:
Prashant Kumar
GitHub: https://github.com/prashant23-kr

ğŸ“œ License
This project is licensed under the MIT License.
You are free to use, modify, and distribute this project.
